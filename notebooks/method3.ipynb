{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Append, instead of Using pd.concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code below is referenced from Lecture_1_2.ipynb provided by Gittu George for DSCI 525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from memory_profiler import memory_usage\n",
    "from os import listdir\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshareairline/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory if missing\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# download missing files\n",
    "files_to_dl = [\"data.zip\"]\n",
    "for item in filter(lambda x: x['name'] in files_to_dl, files):\n",
    "    filename = os.path.join(output_directory, item[\"name\"])\n",
    "    if not os.path.isfile(filename):\n",
    "        urlretrieve(item[\"download_url\"], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all CSVs\n",
    "csvs = glob.glob(output_directory + '*.csv')\n",
    "\n",
    "# As per Tom's guidance, we can exclude the annoying CSV that is formatted differently\n",
    "csvs = [x for x in csvs if \"observed\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 12750.94 MiB, increment: 12652.15 MiB\n",
      "CPU times: user 2min 12s, sys: 37 s, total: 2min 49s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "dat = pd.DataFrame()\n",
    "for file in csvs:\n",
    "    tempdf = pd.read_csv(file)\n",
    "    tempdf['model'] = file.split('_daily')[0]\n",
    "    dat = dat.append(tempdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dat.shape[0] == 62467843"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
